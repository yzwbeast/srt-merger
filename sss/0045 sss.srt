1
00:00:00,060 --> 00:00:00,750
Hey guys,

2
00:00:00,750 --> 00:00:05,330
welcome to Day 45 of 100 Days of Code. Now,

3
00:00:05,330 --> 00:00:08,450
today, we're going to be getting back to coding with Python,

4
00:00:08,900 --> 00:00:12,080
and we're going to be learning how to scrape the web for data

5
00:00:12,320 --> 00:00:14,810
using a module called BeautifulSoup.

6
00:00:15,770 --> 00:00:19,040
Now we've been working with APIs for quite a while now,

7
00:00:19,550 --> 00:00:20,600
and we know that 

8
00:00:20,630 --> 00:00:25,630
we can use a website's API to access their data or to interact with the

9
00:00:26,120 --> 00:00:27,860
website using code.

10
00:00:28,520 --> 00:00:32,810
But some websites don't have an API or their API

11
00:00:32,810 --> 00:00:35,630
doesn't let us do all the things that we want to do.

12
00:00:36,500 --> 00:00:40,460
So this is where we start thinking about using web scraping

13
00:00:41,090 --> 00:00:44,540
where we look through the underlying HTML code

14
00:00:44,570 --> 00:00:48,110
of a website to get hold of the information that we want.

15
00:00:49,130 --> 00:00:52,880
So the aim of today is to learn how to make soup,

16
00:00:53,390 --> 00:00:57,410
but not this kind of soup. We're going to be making BeautifulSoup.

17
00:00:57,980 --> 00:00:59,960
What exactly is BeautifulSoup? Well,

18
00:00:59,990 --> 00:01:04,989
it's a module that helps developers like us make sense of websites.

19
00:01:06,170 --> 00:01:09,770
We could think of a lot of websites as a bit of a spaghetti soup,

20
00:01:10,190 --> 00:01:14,000
even something seemingly as simple as the Google front page,

21
00:01:14,270 --> 00:01:16,850
when you right click on it and view page source,

22
00:01:17,120 --> 00:01:20,060
you can see that it's horrendously complicated.

23
00:01:20,570 --> 00:01:25,100
And if you wanted to make sense of this webpage and pull out the relevant parts

24
00:01:25,100 --> 00:01:25,933
of the data,

25
00:01:26,270 --> 00:01:31,130
then you'll need an HTML parser like BeautifulSoup so that you can

26
00:01:31,130 --> 00:01:36,130
find and pull out the HTML elements that you're interested in from this

27
00:01:36,680 --> 00:01:39,140
soup of jumbled HTML code.

28
00:01:39,770 --> 00:01:42,380
And once we've mastered this skill,

29
00:01:42,500 --> 00:01:46,040
then we'll be able to take any website, for example,

30
00:01:46,070 --> 00:01:48,980
Empire's 100 Greatest Movies Of All Time,

31
00:01:49,220 --> 00:01:53,000
this is a huge list of a hundred movies that apparently everyone should have

32
00:01:53,000 --> 00:01:54,950
watched at some point in their life,

33
00:01:55,400 --> 00:01:58,460
and we can pull out the relevant parts to us

34
00:01:58,700 --> 00:02:02,540
namely the title and the ranking of each movie

35
00:02:02,840 --> 00:02:07,840
and we're going to use it to compile a list of movies that we have to watch so

36
00:02:07,880 --> 00:02:11,390
that we can look at the list, cross out the ones that we've already seen,

37
00:02:11,720 --> 00:02:16,280
and then pick at random one from the list so that we can watch all of the

38
00:02:16,280 --> 00:02:19,550
hundred movies of all time. That's the goal.

39
00:02:19,760 --> 00:02:24,290
And once you're ready head over to the next lesson and we're going to get started

40
00:02:24,350 --> 00:02:25,850
using BeautifulSoup.


41
00:00:00,480 --> 00:00:01,230
In this lesson,

42
00:00:01,230 --> 00:00:06,230
we're going to get started using a library called Beautiful Soup to pass an HTML

43
00:00:06,930 --> 00:00:08,280
file. Parsing

44
00:00:08,280 --> 00:00:13,280
the HTML file is the first step to extracting the data contained in a website. To

45
00:00:13,620 --> 00:00:17,160
get started, the first thing I want you to do is to head over to

46
00:00:17,160 --> 00:00:21,480
the course resources and download the starting project for today.

47
00:00:21,930 --> 00:00:26,010
It's called bs4-start and once you've extracted it and opened it in

48
00:00:26,010 --> 00:00:30,600
PyCharm, then I want you to take a look inside. First, we've got an empty

49
00:00:30,600 --> 00:00:31,800
main.py file

50
00:00:31,830 --> 00:00:36,810
which we're going to write  in order to use Beautiful Soup and extract the data

51
00:00:36,810 --> 00:00:41,670
that we want. And the data is going to come from this website.html.

52
00:00:42,270 --> 00:00:46,590
Now, a really nice thing about PyCharm is when you have an HTML file,

53
00:00:46,620 --> 00:00:49,620
you can always just click on your favorite browser icon here

54
00:00:50,160 --> 00:00:52,770
and it will open up the website as it is.

55
00:00:53,430 --> 00:00:58,430
And you can see that this is a simplified version of our HTML CV page that we

56
00:00:59,130 --> 00:01:02,190
built on day 41. Now,

57
00:01:02,220 --> 00:01:05,550
if you've skipped day 41 to 44

58
00:01:05,580 --> 00:01:08,070
because you already know HTML and CSS,

59
00:01:08,640 --> 00:01:11,190
then have a quick read through the HTML.

60
00:01:11,520 --> 00:01:15,420
It's a very simple document with a bunch of different HTML tags,

61
00:01:15,840 --> 00:01:18,030
attributes like class and ID,

62
00:01:18,570 --> 00:01:23,190
and also a list. I've kept this as simple as possible

63
00:01:23,220 --> 00:01:25,560
just so that it's easy for us to work through it

64
00:01:25,890 --> 00:01:27,600
when we're trying to get hold of things

65
00:01:27,630 --> 00:01:30,840
using Beautiful Soup. In the course resources,

66
00:01:30,900 --> 00:01:34,380
I've also got a link to the Beautiful Soup documentation.

67
00:01:34,860 --> 00:01:39,120
So this is where you can find out everything that you can do with Beautiful 

68
00:01:39,120 --> 00:01:43,920
Soup. But I wanna walk you through some of the most commonly used components.

69
00:01:44,610 --> 00:01:45,360
Beautiful Soup,

70
00:01:45,360 --> 00:01:50,360
as they say, is a Python library for pulling data out of HTML and XML files.

71
00:01:51,420 --> 00:01:56,190
So HTML and XML are both structural languages and they're responsible for

72
00:01:56,190 --> 00:02:00,960
structuring data like the data in a website using these tags.

73
00:02:01,680 --> 00:02:03,330
And the great thing about Beautiful Soup

74
00:02:03,380 --> 00:02:07,850
is it's super easy to use, and it can save you hours or,

75
00:02:07,910 --> 00:02:12,910
days of work to get hold of the data that you want from a particular website.

76
00:02:13,520 --> 00:02:18,410
And you can see the documentation's also been translated by kind users into some

77
00:02:18,440 --> 00:02:21,320
other languages as well. So if you find it easier,

78
00:02:21,590 --> 00:02:24,170
then these languages might help you as well. Now,

79
00:02:24,170 --> 00:02:29,170
the first thing I have to do here in my main.py is to actually get hold of

80
00:02:30,290 --> 00:02:32,270
this particular file.

81
00:02:32,840 --> 00:02:35,600
Now you might remember from previous lessons,

82
00:02:35,660 --> 00:02:38,240
how we open a file in Python.

83
00:02:38,900 --> 00:02:43,900
So have a quick think and see if you can figure out how to get a hold of the

84
00:02:44,330 --> 00:02:49,330
content in this HTML file as a string or as a piece of text.

85
00:02:50,060 --> 00:02:53,840
Pause the video and see if you can complete this challenge. Just as a hint,

86
00:02:53,960 --> 00:02:57,380
you might need the keyword with and the 

87
00:02:57,380 --> 00:02:58,330
keyword open.

88
00:03:01,030 --> 00:03:04,660
All right. So the way we do this, as we say with open,

89
00:03:05,020 --> 00:03:09,010
and then we provide the name of the file, which is website.html.

90
00:03:09,520 --> 00:03:13,540
And we can open this as a alias name which we'll call file,

91
00:03:14,140 --> 00:03:18,580
and now we have access to this file and we can say file.read.

92
00:03:19,240 --> 00:03:21,010
Now, once we've read this file,

93
00:03:21,040 --> 00:03:24,730
we can save this to a variable

94
00:03:24,730 --> 00:03:29,590
which we'll call contents. And once we've gotten hold of these contents,

95
00:03:29,770 --> 00:03:32,740
then we can start using Beautiful Soup.

96
00:03:33,460 --> 00:03:36,940
So as always, we always start with the import,

97
00:03:37,120 --> 00:03:42,120
so the name of the library that we're going to install is called bs4

98
00:03:43,600 --> 00:03:45,850
and this is Beautiful Soup version four

99
00:03:45,970 --> 00:03:49,210
which is currently the latest version of Beautiful Soup.

100
00:03:49,870 --> 00:03:53,620
And from this particular package, we want to import the class

101
00:03:53,620 --> 00:03:57,130
which is called Beautiful Soup. Now,

102
00:03:57,160 --> 00:04:00,460
if you downloaded this project from the course resources,

103
00:04:00,820 --> 00:04:05,820
you should see that there's no errors underlining this line because bs4 has

104
00:04:06,430 --> 00:04:09,610
already been installed. If you see some squiggly underlines,

105
00:04:09,670 --> 00:04:13,660
just click on the red light bulb and install the required module

106
00:04:13,750 --> 00:04:17,230
which is called bs4. Now,

107
00:04:17,230 --> 00:04:20,380
once we've got hold of our BeautifulSoup class

108
00:04:20,500 --> 00:04:24,220
then we're ready to make soup. In order to make soup,

109
00:04:24,280 --> 00:04:29,280
we use our BeautifulSoup class and we create a new object from that class

110
00:04:29,770 --> 00:04:34,360
and we pass in a string. So this is the markup,

111
00:04:34,720 --> 00:04:39,460
and that's the same M that you find in HTML and XML.

112
00:04:39,760 --> 00:04:44,760
It's the hypertext markup language and the extensible markup language.

113
00:04:47,170 --> 00:04:51,580
So the markup refers to basically all of this.

114
00:04:52,570 --> 00:04:55,480
We've already gotten hold of it through this contents

115
00:04:55,720 --> 00:04:57,520
so let's go ahead and pass that in.

116
00:04:58,300 --> 00:05:02,680
So now that we've specified what it is we want to use to create our soup,

117
00:05:03,370 --> 00:05:06,340
the next thing we have to provide is the parser.

118
00:05:06,850 --> 00:05:09,370
This is going to help the BeautifulSoup module

119
00:05:09,430 --> 00:05:14,430
understand what language this particular content is structured in.

120
00:05:15,370 --> 00:05:18,490
As they mentioned, it can parse HTML and XML

121
00:05:18,700 --> 00:05:22,660
so we have to tell it what particular type of document we've got.

122
00:05:23,260 --> 00:05:26,920
And the easiest way is just to use the Python

123
00:05:27,040 --> 00:05:28,420
html.parser.

124
00:05:29,650 --> 00:05:34,030
So after we've parsed in the text that we want to turn into soup,

125
00:05:34,600 --> 00:05:39,600
we're going to add the parser as html.parser.

126
00:05:41,740 --> 00:05:45,940
And this is going to help Beautiful Soup understand these contents.

127
00:05:47,200 --> 00:05:50,590
Now, depending on the website that you're working with, occasionally,

128
00:05:50,590 --> 00:05:55,390
you might need to use the lxml's parser. And do that,

129
00:05:55,480 --> 00:05:59,990
all you have to do is say import lxml

130
00:06:00,290 --> 00:06:05,290
and then install this particular package, and here in a string

131
00:06:06,290 --> 00:06:10,730
instead of using html.parser, you can use lxml.

132
00:06:11,450 --> 00:06:16,400
And this is basically just a different way of parsing or understanding the

133
00:06:16,400 --> 00:06:18,380
content that you're passing to Beautiful Soup.

134
00:06:18,890 --> 00:06:23,750
And I found that with certain websites the html.parser might not work

135
00:06:23,750 --> 00:06:26,990
and you might get an error that tells you something about your parser not

136
00:06:26,990 --> 00:06:31,070
working. So then you might consider using lxml instead.

137
00:06:32,450 --> 00:06:36,710
So this one line of code basically completes our parsing.

138
00:06:37,280 --> 00:06:42,280
And this soup is now an object that allows us to tap in to various parts of the

139
00:06:43,550 --> 00:06:47,270
website, but using Python code. For example,

140
00:06:47,270 --> 00:06:51,470
if I wanted this title tag out of this whole website,

141
00:06:51,770 --> 00:06:55,610
all I have to do is say soup.title.

142
00:06:57,680 --> 00:07:01,760
And now if I print this soup.title,

143
00:07:02,360 --> 00:07:07,360
you can see that we'll get the title tag being printed out in its entirety.

144
00:07:09,710 --> 00:07:14,710
Once Beautiful Soup has made sense of this website by parsing the HTML,

145
00:07:17,300 --> 00:07:19,460
we can now tap into that object

146
00:07:19,520 --> 00:07:23,510
which is the HTML code as if it were a Python object.

147
00:07:23,840 --> 00:07:25,730
So we can tap into the title,

148
00:07:26,000 --> 00:07:30,200
but we can dig even deeper. Instead of just tapping into the title,

149
00:07:30,500 --> 00:07:35,500
we can also get hold of other things like the title.name,

150
00:07:36,200 --> 00:07:41,200
and this is going to give us the name of that particular title tag.

151
00:07:41,960 --> 00:07:44,900
So remember this gave us the title tag,

152
00:07:44,930 --> 00:07:49,930
so all of this, and this next stage drilling even deeper into the name of this

153
00:07:50,900 --> 00:07:54,200
tag, you'll see that it gives us title.

154
00:07:54,710 --> 00:07:57,590
So the name of this title tag is called title,

155
00:07:58,250 --> 00:08:01,460
and we can also get hold of the string

156
00:08:01,700 --> 00:08:06,700
which is contained in the title tag by simply using .title.string.

157
00:08:07,280 --> 00:08:11,840
And you can see this is the actual string that's inside that title tag.

158
00:08:13,280 --> 00:08:16,130
If we think about it, this entire soup

159
00:08:16,190 --> 00:08:19,520
object now represents our HTML code.

160
00:08:20,030 --> 00:08:24,440
So I can also actually just print out the entire soup object.

161
00:08:24,950 --> 00:08:29,180
And you can see that this is basically just all HTML.

162
00:08:29,990 --> 00:08:34,100
And if you want to, there's even a method called prettify

163
00:08:34,460 --> 00:08:37,820
which will indent your soup HTML code.

164
00:08:38,120 --> 00:08:43,120
So now, compared this where everything's all on one line, with this prettified

165
00:08:43,909 --> 00:08:47,900
version where everything's all indented properly and easier to read.

166
00:08:49,130 --> 00:08:53,780
In addition to getting the title tag, we can also get hold of,

167
00:08:53,810 --> 00:08:56,700
for example, the a tag.

168
00:08:57,060 --> 00:09:02,060
So this is going to give us the first anchor tag that it finds in our website,

169
00:09:02,700 --> 00:09:05,220
which happens to be this one right here.

170
00:09:06,180 --> 00:09:11,180
And we can swap that with maybe the first li or the first paragraph.

171
00:09:14,910 --> 00:09:18,450
Essentially what we're doing with beautiful soup is we're just drilling down

172
00:09:18,480 --> 00:09:23,370
into this HTML file, finding the HTML tags that we're interested in,

173
00:09:23,850 --> 00:09:26,040
and then getting hold of the

174
00:09:26,100 --> 00:09:30,660
either name of the tag or the actual text of the tag.

175
00:09:31,470 --> 00:09:36,470
But what if we wanted all of the paragraphs or all of the anchor tags in our

176
00:09:37,380 --> 00:09:41,880
website, how would we do that? In the next lesson,

177
00:09:42,030 --> 00:09:47,030
we're going to dive deeper into searching through websites for all of the

178
00:09:47,190 --> 00:09:49,500
components that we're looking for. For example,

179
00:09:49,740 --> 00:09:52,410
all of the P tags or all of the anchor tags.

180
00:09:52,770 --> 00:09:57,770
And we're going to see how we can refine our search and specify exactly what it

181
00:09:58,140 --> 00:10:02,610
is that we want. So for all of that and more, I'll see you on the next lesson.


182
00:00:00,210 --> 00:00:02,190
Now there's two things you'll notice here.

183
00:00:02,580 --> 00:00:06,900
One is we're only getting hold of the first, for example,

184
00:00:06,900 --> 00:00:11,730
p tag or a tag, but we're not getting hold of any of the other ones.

185
00:00:12,120 --> 00:00:14,970
So what if we wanted to get all of the anchor tags,

186
00:00:15,000 --> 00:00:18,420
all of the paragraphs in our website? Well,

187
00:00:18,450 --> 00:00:23,450
then we can use a function that comes with Beautiful Soup called find_all.

188
00:00:24,570 --> 00:00:27,990
This is probably one of the most commonly used methods when it comes to

189
00:00:27,990 --> 00:00:28,823
Beautiful Soup 

190
00:00:29,730 --> 00:00:34,730
And here we can search by a bunch of things. We could search by name,

191
00:00:34,950 --> 00:00:35,783
so we can say

192
00:00:35,790 --> 00:00:40,790
find all of the tags where the tag name is equal to a.

193
00:00:43,080 --> 00:00:46,050
So this is going to give us all of the anchor tags,

194
00:00:46,190 --> 00:00:47,023
right?

195
00:00:49,220 --> 00:00:50,840
And if I print that,

196
00:00:51,140 --> 00:00:56,140
you can see that it gives us a list and it gives us all three of the links that

197
00:00:57,650 --> 00:00:59,450
exists in our website.

198
00:01:00,350 --> 00:01:03,290
And if I change this to p for example,

199
00:01:03,470 --> 00:01:08,470
then it'll find all of the paragraphs and we can change this to basically any of

200
00:01:09,650 --> 00:01:13,850
the tag names in our website. Now,

201
00:01:13,850 --> 00:01:16,280
what if we wanted to drill a little bit deeper?

202
00:01:16,820 --> 00:01:19,190
We've got a list of all the anchor tags,

203
00:01:19,490 --> 00:01:23,870
but what if I only wanted the text in that anchor tag?

204
00:01:23,870 --> 00:01:28,730
So I just wanted this part. Well, how would I get hold of all of them? Well,

205
00:01:28,760 --> 00:01:31,730
firstly, we would probably need a for loop.

206
00:01:32,030 --> 00:01:37,030
So we could say for tag in all anchor tags and we can loop through all of those

207
00:01:39,560 --> 00:01:44,560
anchor tags and use a method called tag.getText.

208
00:01:46,220 --> 00:01:49,040
And now if I go ahead and print this,

209
00:01:49,130 --> 00:01:54,130
you can see that it's basically going to print out all three of the text that is

210
00:01:55,490 --> 00:01:59,810
in all three of the anchor tags that it found. Now,

211
00:01:59,810 --> 00:02:02,420
what if I didn't want to get the text,

212
00:02:02,510 --> 00:02:07,340
but instead I wanted to get hold of the actual href,

213
00:02:07,370 --> 00:02:11,870
so the link, right? So let's print our all_anchor_tags again.

214
00:02:12,650 --> 00:02:13,483
All right.

215
00:02:15,800 --> 00:02:20,150
And you can see that there is a attribute called href

216
00:02:20,420 --> 00:02:25,190
which stores the actual link that the tag goes to.

217
00:02:25,700 --> 00:02:30,200
So very often, you'll want to isolate that link. So how would you do that?

218
00:02:30,800 --> 00:02:31,280
Well,

219
00:02:31,280 --> 00:02:36,280
you can tap into each of the tags and you can use another method called get.

220
00:02:38,390 --> 00:02:42,140
And here you can get the value of any of the attributes.

221
00:02:42,590 --> 00:02:46,250
So if I pass in href here and I print this,

222
00:02:46,940 --> 00:02:51,200
then it's going to give me all of the links and it's not going to give me

223
00:02:51,200 --> 00:02:54,080
anything else. It's basically just stripped out the link

224
00:02:54,200 --> 00:02:59,090
which is what I'm interested in. Similarly, when we use find_all

225
00:02:59,920 --> 00:03:04,920
we can also find things by their attribute, so the moment we're searching by

226
00:03:05,410 --> 00:03:08,950
the tag name, but we can also get hold of things

227
00:03:09,040 --> 00:03:11,860
by the attribute name. For example,

228
00:03:11,860 --> 00:03:16,780
if I wanted to get hold of this item, I can of course search for an h1.

229
00:03:17,080 --> 00:03:19,660
But what if I had lots of h1s? Well,

230
00:03:19,660 --> 00:03:24,430
then I could isolate it by this ID. So I could say,

231
00:03:27,210 --> 00:03:27,990
soup

232
00:03:27,990 --> 00:03:32,990
.find_all which will give me a list of all of the items that match the search

233
00:03:34,680 --> 00:03:35,513
query,

234
00:03:35,760 --> 00:03:40,760
or I can use the find method to only find the first item that matches the query.

235
00:03:42,900 --> 00:03:45,450
In my case, there's only one thing I'm looking for.

236
00:03:45,690 --> 00:03:50,610
And this particular tag has a name of h1

237
00:03:51,270 --> 00:03:56,270
but it's also got a ID of name.

238
00:03:58,440 --> 00:03:59,280
As you can see,

239
00:03:59,520 --> 00:04:03,510
this ID is equal to name and it's also an h1 tag.

240
00:04:04,110 --> 00:04:07,290
So this will give us that particular element.

241
00:04:07,380 --> 00:04:12,380
So if I print out this heading and let's comment out everything else,

242
00:04:16,920 --> 00:04:21,600
then now you can see I've just isolated that one h1.

243
00:04:22,260 --> 00:04:25,830
And this also means if I just add another h1 here,

244
00:04:26,130 --> 00:04:26,963
...

245
00:04:29,220 --> 00:04:31,080
and I run this code again,

246
00:04:31,350 --> 00:04:36,150
that is not going to show up because I've said it has to have a name of h1

247
00:04:36,570 --> 00:04:40,020
and an ID that matches this particular value.

248
00:04:41,670 --> 00:04:42,900
Now, as you can imagine,

249
00:04:42,900 --> 00:04:47,040
you can also do the same thing with the class attribute.

250
00:04:47,640 --> 00:04:49,110
So we can say,

251
00:04:52,920 --> 00:04:53,430
...

252
00:04:53,430 --> 00:04:56,520
soup.find because again, I'm only looking for one.

253
00:04:57,240 --> 00:05:01,290
And the thing that I'm looking for has a name

254
00:05:01,680 --> 00:05:04,290
which is a h3

255
00:05:04,380 --> 00:05:05,213
...

256
00:05:07,140 --> 00:05:12,140
but it's also got a class that's equal to heading.

257
00:05:13,200 --> 00:05:17,220
So I'm just going to copy that and paste that in here. Now,

258
00:05:17,250 --> 00:05:22,250
one of the things you'll get here is an error because this class keyword is a

259
00:05:23,700 --> 00:05:25,710
reserved keyword in Python.

260
00:05:26,190 --> 00:05:29,100
And what that means is that it's a special word

261
00:05:29,370 --> 00:05:34,170
which can only be used for creating classes. Now, in this case,

262
00:05:34,200 --> 00:05:38,100
we're definitely not creating a class or an object here. Instead,

263
00:05:38,100 --> 00:05:39,990
we're trying to tap into an attribute.

264
00:05:40,470 --> 00:05:43,410
So in order to not clash with the class keyword,

265
00:05:43,680 --> 00:05:46,980
this attribute is actually called class_re.

266
00:05:48,330 --> 00:05:52,920
Now it's going to look for all of the h3s where the class attribute is

267
00:05:52,920 --> 00:05:54,360
equal to heading.

268
00:05:54,970 --> 00:05:59,970
Let's go ahead and print this section_heading and you should see now we'll get that 

269
00:06:01,220 --> 00:06:05,810
h3 with the class of heading show up. And again,

270
00:06:05,810 --> 00:06:09,080
if we wanted to get hold of the text

271
00:06:09,110 --> 00:06:14,060
that's contained in that h3, then we simply use the getText method,

272
00:06:14,600 --> 00:06:19,490
or if we want to know the name of that particular tag,

273
00:06:19,550 --> 00:06:21,080
then we can say .name.

274
00:06:23,920 --> 00:06:24,370
Okay.

275
00:06:24,370 --> 00:06:29,230
And if we want to get hold of the value of an attribute, for example,

276
00:06:29,260 --> 00:06:32,680
get the class value,

277
00:06:32,830 --> 00:06:36,430
then we can do something like this. Now,

278
00:06:36,460 --> 00:06:41,460
while that's a pretty good way of selecting elements from the entire website,

279
00:06:42,130 --> 00:06:46,360
there's certain cases where it might not work. For example,

280
00:06:46,990 --> 00:06:51,100
at the moment here, we've got our three anchor tags.

281
00:06:51,640 --> 00:06:55,300
If we wanted to get hold of a specific anchor tag,

282
00:06:55,540 --> 00:06:59,950
let's say we wanted this anchor tag, then what do we do?

283
00:07:00,340 --> 00:07:00,640
Well,

284
00:07:00,640 --> 00:07:05,640
then we could just simply find all of the anchor tags and then find the first

285
00:07:06,820 --> 00:07:09,280
one. But as you can imagine,

286
00:07:09,310 --> 00:07:12,400
this is a incredibly simple website.

287
00:07:12,700 --> 00:07:15,490
Most websites will have thousands

288
00:07:15,520 --> 00:07:19,000
if not tens of thousands of links. In that situation,

289
00:07:19,180 --> 00:07:24,180
it's really hard to know which particular link you want from the list of all of

290
00:07:24,880 --> 00:07:25,713
the anchor tags.

291
00:07:26,320 --> 00:07:31,320
So we want to have a way where we can drill down into a particular element.

292
00:07:32,440 --> 00:07:35,770
What's unique about this particular anchor tag? Well,

293
00:07:35,800 --> 00:07:40,800
it sits inside a strong tag and it sits inside an emphasis tag and it sits

294
00:07:41,830 --> 00:07:46,390
inside a paragraph tag, which itself is in the body.

295
00:07:47,080 --> 00:07:52,080
We can narrow it down using these steps. In our current website,

296
00:07:52,540 --> 00:07:57,340
nowhere else is there an anchor tag that sits inside a paragraph tag.

297
00:07:57,940 --> 00:08:02,940
And you'll remember from our previous lessons on CSS that you can use CSS

298
00:08:03,670 --> 00:08:08,670
selectors in order to narrow down on a particular element in order to specify

299
00:08:09,580 --> 00:08:13,600
its style. And if we were to write CSS code,

300
00:08:15,370 --> 00:08:17,680
then it would look something like this.

301
00:08:19,000 --> 00:08:24,000
So we would select first the paragraph and then we would select the anchor tag

302
00:08:24,670 --> 00:08:29,670
and then we can specify what the style should be

303
00:08:32,200 --> 00:08:32,830
.

304
00:08:32,830 --> 00:08:34,690
Now. When, we're using  Beautiful Soup,

305
00:08:34,840 --> 00:08:37,990
we can also use the CSS selectors.

306
00:08:38,620 --> 00:08:43,270
I can get hold of that company URL by simply saying soup,

307
00:08:43,720 --> 00:08:48,370
and instead of using find or find_all, I'm going to use select_one.

308
00:08:49,210 --> 00:08:54,210
There's select and select_one. Select_one will give us the first matching

309
00:08:54,430 --> 00:08:58,620
item and select will give us all of the matching items in a list.

310
00:08:59,280 --> 00:09:04,050
Now we get to specify the selector as a string. And again,

311
00:09:04,080 --> 00:09:07,020
I'm going to use the same selector that I showed you before.

312
00:09:07,350 --> 00:09:11,820
So we're looking for a a tag  which sits inside p tag.

313
00:09:12,240 --> 00:09:17,040
And this string is the CSS selector. So you can write anything in here

314
00:09:17,040 --> 00:09:21,810
really. This means that we'll be able to get that anchor tag.

315
00:09:22,080 --> 00:09:27,080
And then once I've gotten hold of the company URL and print it out,

316
00:09:27,720 --> 00:09:31,290
you can see its that exact anchor tag that we wanted.

317
00:09:32,610 --> 00:09:36,270
We don't have to just stick to the HTML selectors.

318
00:09:36,300 --> 00:09:40,890
You can also use the class or the ID in your CSS selector.

319
00:09:41,280 --> 00:09:44,220
So remember, to select on an ID,

320
00:09:44,520 --> 00:09:47,550
we use the pound sign. So let's say

321
00:09:47,880 --> 00:09:51,720
we want to get hold of this h1, which has an ID of name,

322
00:09:51,990 --> 00:09:53,820
we can say #name,

323
00:09:54,180 --> 00:09:58,740
and now this is going to be equal to my name.

324
00:09:59,160 --> 00:10:01,740
And if I now run it, you can see that last one,

325
00:10:01,950 --> 00:10:06,570
the element that was picked out is the h1 with the ID of name.

326
00:10:07,650 --> 00:10:12,210
And finally you can use a CSS selector to select an element by class.

327
00:10:12,390 --> 00:10:16,680
So for example, here, we've got heading and here we've got heading as well.

328
00:10:17,160 --> 00:10:22,160
So if we want to select all of the elements that have a class of heading,

329
00:10:22,980 --> 00:10:26,940
then we could say soup.select so this will give us a list

330
00:10:27,060 --> 00:10:32,060
and then the selector is the first item that goes into the method.

331
00:10:32,520 --> 00:10:37,470
So similar to this, we can have this keyword argument there or we can delete it.

332
00:10:38,070 --> 00:10:43,070
And this selector will be using the .heading in order to select the element

333
00:10:45,060 --> 00:10:47,130
that has a class of heading.

334
00:10:49,410 --> 00:10:53,430
And this is now going to be a list if we print it out.

335
00:10:55,620 --> 00:10:56,580
Right here.

336
00:10:56,630 --> 00:10:58,530
So,

337
00:10:58,880 --> 00:11:03,500
you can use everything that you've learned about CSS selectors to select a

338
00:11:03,500 --> 00:11:06,560
particular item out of an HTML file.

339
00:11:07,040 --> 00:11:11,330
And this is usually really useful because a lot of these elements will be nested

340
00:11:11,330 --> 00:11:14,600
inside divs and the div will have an ID

341
00:11:14,870 --> 00:11:19,190
and then all you have to do is to narrow down on the div and then narrow down on the

342
00:11:19,190 --> 00:11:20,023
element you want.

343
00:11:20,240 --> 00:11:25,240
So you can basically drill through using CSS selectors to get to any item you

344
00:11:26,180 --> 00:11:27,260
want on the page.

345
00:11:28,460 --> 00:11:33,460
Now that we've looked at how to find various items from HTML using Beautiful

346
00:11:33,980 --> 00:11:37,610
Soup, in the next lesson, I've got a quiz for you

347
00:11:37,910 --> 00:11:42,910
for you to have a go and have some practice at selecting and finding elements

348
00:11:43,850 --> 00:11:46,760
from an HTML file using Beautiful Soup.

349
00:11:47,240 --> 00:11:50,330
So for all of that and more, head over to the next lesson.


350
00:00:00,420 --> 00:00:05,420
In the last lesson, we got started using Beautiful Soup and we saw how we could

351
00:00:05,430 --> 00:00:10,430
use it to parse through the HTML of a website and pull out the pieces that we're

352
00:00:10,560 --> 00:00:11,393
interested in.

353
00:00:12,210 --> 00:00:16,920
Now it's no fun scraping a website that you've already got access to locally.

354
00:00:17,490 --> 00:00:21,900
It's much better if we can get hold of something that's currently live on the

355
00:00:21,900 --> 00:00:26,520
internet. So I'm going to go ahead and comment out all of this code,

356
00:00:27,960 --> 00:00:32,960
and I'm going to be using Beautiful Soup to get hold of a live website and grab

357
00:00:33,900 --> 00:00:34,800
data from it.

358
00:00:35,580 --> 00:00:40,170
And the website that we're going to be using is the YCombinator's Hacker

359
00:00:40,170 --> 00:00:41,003
News website.

360
00:00:41,670 --> 00:00:46,650
This is where anybody can post a link to a news piece that they've discovered

361
00:00:46,650 --> 00:00:51,300
that's tech related, or you could show off things that you've built.

362
00:00:51,660 --> 00:00:52,200
For example,

363
00:00:52,200 --> 00:00:56,430
I've just been looking at this guy's website that he built called My Desk Tour

364
00:00:57,030 --> 00:00:59,760
where you can post a picture of your desk setup,

365
00:01:00,150 --> 00:01:04,110
and you can see all of the tools and gear that they've got.

366
00:01:04,860 --> 00:01:08,160
So we're going to be scraping the main Hacker News website

367
00:01:08,190 --> 00:01:10,980
which is under this particular URL.

368
00:01:11,520 --> 00:01:15,450
And this is usually where I go to find the latest tech news.

369
00:01:16,200 --> 00:01:20,760
We're going to copy this URL, and we're going to go back to our main.py.

370
00:01:21,270 --> 00:01:25,050
And in order to download the data from that website,

371
00:01:25,080 --> 00:01:28,860
we're going to be using our handy friend, which is requests.

372
00:01:29,520 --> 00:01:34,520
Now requests allows us to get hold off of the data from a particular URL,

373
00:01:36,060 --> 00:01:39,240
which in this case is news.ycombinator.com.

374
00:01:40,500 --> 00:01:43,080
And once we've made that request,

375
00:01:43,110 --> 00:01:48,110
then we can save the data that we get back in a response variable.

376
00:01:49,070 --> 00:01:50,840
And once we've got the response,

377
00:01:50,870 --> 00:01:54,350
we can actually print out the text of the response

378
00:01:54,890 --> 00:01:59,890
and this is basically equivalent to what we did when we opened up our HTML file

379
00:02:00,590 --> 00:02:04,040
and we read the file contents, the text of the file.

380
00:02:04,700 --> 00:02:06,950
So now if I go ahead and run this,

381
00:02:07,010 --> 00:02:10,850
then you can see that it's going to print out loads of stuff,

382
00:02:11,150 --> 00:02:15,860
but this is basically the code that represents this particular page.

383
00:02:16,310 --> 00:02:19,700
So in fact, if you right click and click view page source,

384
00:02:19,730 --> 00:02:24,470
you'll see that this is exactly the same HTML code that we're getting back over

385
00:02:24,470 --> 00:02:28,790
here. So we don't actually want all of this jumbled mess.

386
00:02:29,180 --> 00:02:34,180
What we're more interested in is the specific titles and the links

387
00:02:34,760 --> 00:02:37,100
for each of these pieces.

388
00:02:37,730 --> 00:02:41,480
It shows by default 30 of the top articles,

389
00:02:41,870 --> 00:02:43,940
and this is ranked by an algorithm.

390
00:02:43,970 --> 00:02:48,970
So it's most recent and also getting a lot of traction,

391
00:02:49,040 --> 00:02:50,420
so a lot of upvotes,

392
00:02:50,840 --> 00:02:55,730
but it doesn't represent the most upvoted items. So you can see here, in fact,

393
00:02:55,760 --> 00:02:58,340
the most upvoted at least today anyways,

394
00:02:58,430 --> 00:03:01,210
is Mozilla laying off of 250 employees.

395
00:03:01,960 --> 00:03:06,960
So what if I wanted to get hold of the article title and the link of the post

396
00:03:07,840 --> 00:03:10,300
from this page that has the highest point.

397
00:03:10,690 --> 00:03:14,590
I don't want to have to manually check all of this. I want to do it with code.

398
00:03:15,280 --> 00:03:18,040
So let's go ahead and scrape it. Now,

399
00:03:18,070 --> 00:03:21,970
if I right click on each of these titles and I click on inspect,

400
00:03:22,420 --> 00:03:26,530
then it takes me to the precise line of code in the HTML

401
00:03:26,560 --> 00:03:30,070
that's responsible for rendering this component.

402
00:03:30,550 --> 00:03:34,210
This is actually a anchor tag, so that's the

403
00:03:34,240 --> 00:03:39,240
a tag. And the text in the anchor tag is the title of the article,

404
00:03:41,470 --> 00:03:45,430
and then the href is the link that will take me to the actual story. So

405
00:03:45,460 --> 00:03:47,230
if I click on this, you can see

406
00:03:47,230 --> 00:03:51,820
it takes me to the actual news piece about Joan Feynman.

407
00:03:52,960 --> 00:03:54,880
So what about this point? Well,

408
00:03:54,880 --> 00:03:57,850
let's go ahead and right click on it and click inspect.

409
00:03:58,180 --> 00:04:03,180
You can see this is in a span and it has a class of score while this title is in

410
00:04:06,910 --> 00:04:10,570
a a ref and it has a class of story link.

411
00:04:11,170 --> 00:04:13,420
So with those two pieces of information,

412
00:04:13,570 --> 00:04:17,350
we can use Beautiful Soup to scrape all of the titles,

413
00:04:17,500 --> 00:04:20,019
all of the links and all of their points.

414
00:04:20,470 --> 00:04:24,550
And we can compare all those points and figure out which one has the highest

415
00:04:24,550 --> 00:04:28,030
point on this page. So let's go ahead and do that.

416
00:04:29,020 --> 00:04:33,310
So I'm gonna save the yc_webpage as the response.text,

417
00:04:34,300 --> 00:04:38,710
and then I'm going to use Beautiful Soup to parse that webpage.

418
00:04:39,220 --> 00:04:40,480
So BeautifulSoup

419
00:04:40,540 --> 00:04:45,540
and then I'm going to pass in the actual HTML document that we want to parse.

420
00:04:45,910 --> 00:04:48,220
So this is the YC webpage,

421
00:04:48,820 --> 00:04:52,750
and then we provide the method to which we're going to parse it.

422
00:04:52,750 --> 00:04:56,890
So html.parser, with an ER at the end.

423
00:04:57,580 --> 00:05:01,930
And this is our soup. Once we've created our soup,

424
00:05:02,620 --> 00:05:06,580
the next step is actually to dig in the soup and find the parts that we want.

425
00:05:06,730 --> 00:05:11,290
So if, for example, if I want to get hold of the title of all of that,

426
00:05:11,290 --> 00:05:16,060
then I can just say print soup.title. And now you'll see

427
00:05:16,060 --> 00:05:19,000
it gives me the title which is Hacker News.

428
00:05:19,780 --> 00:05:24,100
And that's the same as what you see here in the tab bar.

429
00:05:24,910 --> 00:05:26,710
Now, what if I didn't want the title?

430
00:05:26,710 --> 00:05:31,150
What if I actually wanted to get hold of this text here,

431
00:05:31,750 --> 00:05:34,120
the title of each of these articles?

432
00:05:34,810 --> 00:05:39,810
See if you can figure out how to get hold of this text and print it out in your

433
00:05:40,330 --> 00:05:44,590
code. Remember it has the class that's a story link,

434
00:05:44,950 --> 00:05:46,360
and it's an anchor tag.

435
00:05:46,960 --> 00:05:50,410
Pause the video and see if you can get this title,

436
00:05:50,590 --> 00:05:53,380
so yours might be different from what I've got on screen of course.

437
00:05:53,440 --> 00:05:57,590
It depends on what's showing up on Hacker News on the day you are doing this.

438
00:05:58,010 --> 00:06:02,810
But get the title of the first article printed out using BeautifulSoup.

439
00:06:03,490 --> 00:06:04,323
Okay.

440
00:06:05,800 --> 00:06:08,920
All right. What we want to do is we want to use find.

441
00:06:09,430 --> 00:06:14,430
So we're going to find the first instance from this webpage where the actual

442
00:06:15,850 --> 00:06:20,080
name of the tag is equal to a, so that's an anchor tag,

443
00:06:20,770 --> 00:06:25,770
and then the class is equal to the story link.

444
00:06:27,250 --> 00:06:29,530
So I'm just gonna copy that and paste it in.

445
00:06:30,220 --> 00:06:34,600
Remember that in order to not clash with the reserved class keyword,

446
00:06:34,630 --> 00:06:37,210
we have to add a underscore afterwards.

447
00:06:38,200 --> 00:06:41,350
Now this should be our article tag.

448
00:06:42,100 --> 00:06:44,230
And if we go ahead and print it,

449
00:06:44,470 --> 00:06:49,470
then you can see that we get this exact anchor tag.

450
00:06:50,350 --> 00:06:54,130
But if we want to get hold of the text that's actually in the anchor tag,

451
00:06:54,190 --> 00:06:58,450
then we have to go one step further and call the getText method

452
00:06:58,570 --> 00:07:03,190
that's also from Beautiful Soup. So now when I run that, you can see

453
00:07:03,310 --> 00:07:06,670
I only get the actual text of the article.

454
00:07:07,510 --> 00:07:12,310
Let's work on some of the other pieces. So this is the article text.

455
00:07:13,840 --> 00:07:18,840
And then if we want to get hold of the article_link and the article_upvotes.

456
00:07:22,300 --> 00:07:26,020
See if you can figure out how to complete these two parts as well.

457
00:07:26,200 --> 00:07:30,880
So we want the HTML link that is, of course, all of this HTTP,

458
00:07:30,880 --> 00:07:34,330
et cetera. And then we also want to get hold of the upvote

459
00:07:34,390 --> 00:07:37,150
which is this little number right here.

460
00:07:38,110 --> 00:07:41,080
It's inside a span with a class of score.

461
00:07:41,380 --> 00:07:42,213
Right?

462
00:07:44,500 --> 00:07:48,190
All right. So let's do the first thing, which is article link. Well,

463
00:07:48,190 --> 00:07:52,840
we can actually already tap into the same article tag we already got up here.

464
00:07:53,260 --> 00:07:55,060
And instead of saying getText,

465
00:07:55,090 --> 00:08:00,090
we can use the get method to get the specific value of a attribute.

466
00:08:02,080 --> 00:08:04,990
So what we want is of course, the href.

467
00:08:06,520 --> 00:08:09,190
And then the article_upvote,

468
00:08:09,250 --> 00:08:14,250
we'll have to tap into our soup and find the tag with a name that is span

469
00:08:17,140 --> 00:08:21,400
because this is what we're looking for, and has a class of score.

470
00:08:21,880 --> 00:08:22,713
Right?

471
00:08:24,400 --> 00:08:29,400
Like this. Finding this particular tag is not enough.

472
00:08:29,680 --> 00:08:34,570
This actually just gets us the tag. If we want to go further

473
00:08:34,570 --> 00:08:38,350
and we actually want to get the text that's inside that span

474
00:08:38,650 --> 00:08:41,230
which is of course the 19 points,

475
00:08:41,799 --> 00:08:45,280
then we have to dig one step deeper and call the

476
00:08:45,310 --> 00:08:49,450
getText method like this. Now,

477
00:08:49,480 --> 00:08:54,480
if I go ahead and print out the article_text and the article_link,

478
00:08:55,710 --> 00:08:59,550
and also finally the article_upvote,

479
00:09:00,090 --> 00:09:01,200
then you can see

480
00:09:01,200 --> 00:09:05,730
I get all three pieces of data that I'm interested in. Now,

481
00:09:05,790 --> 00:09:10,620
instead of getting the first occurrence, I want to get all of the ones that are

482
00:09:10,650 --> 00:09:15,570
on this page, so all 30 results. Now, in order to do that,

483
00:09:15,810 --> 00:09:20,810
I have to change the find to find_all both here and here.

484
00:09:23,280 --> 00:09:23,790
This way

485
00:09:23,790 --> 00:09:28,790
we get a list of all of the articles and I'll get a list of all of the article

486
00:09:31,860 --> 00:09:32,693
_upvotes.

487
00:09:33,990 --> 00:09:38,040
So now it's going to be a little bit different. In order to get all of the text

488
00:09:38,070 --> 00:09:41,730
and all of the link, then I have to use a for loop.

489
00:09:42,450 --> 00:09:46,410
So I'll say for article tag in articles,

490
00:09:46,740 --> 00:09:47,940
so articles is of course,

491
00:09:47,940 --> 00:09:52,410
this list where we find all of the anchor tags with a class of storylink,

492
00:09:53,130 --> 00:09:56,820
and then I'm going to loop through each one of those and for each of the tags,

493
00:09:56,850 --> 00:10:00,030
I'm going to get the text and also get the Href.

494
00:10:02,100 --> 00:10:06,150
I'm going to create two new lists, articles_text, and article_links.

495
00:10:08,550 --> 00:10:12,930
And then I'm going to save each of the new articles into those lists.

496
00:10:19,700 --> 00:10:20,533
...

497
00:10:20,690 --> 00:10:23,000
Like this. And in fact,

498
00:10:23,030 --> 00:10:28,030
we could probably simplify this a little bit by refactoring and renaming the

499
00:10:30,020 --> 00:10:31,040
article text,

500
00:10:31,070 --> 00:10:35,240
so the singular version into just text and the article_link

501
00:10:36,770 --> 00:10:38,420
to just the link.

502
00:10:39,740 --> 00:10:44,300
So now let's print out the lists. So the article_texts,

503
00:10:45,530 --> 00:10:49,010
the article_links and the article_upvotes.

504
00:10:49,760 --> 00:10:51,440
And this find_all

505
00:10:51,440 --> 00:10:55,760
gives me a list and I can't call getText on the list.

506
00:10:56,120 --> 00:10:59,720
So I'll also need to create a new list.

507
00:11:00,050 --> 00:11:02,360
So I'm going to choose to use list comprehension here.

508
00:11:03,260 --> 00:11:08,260
So I'm going to say for score in all of the scores,

509
00:11:10,370 --> 00:11:13,850
we're going to create a list using each of those scores

510
00:11:14,090 --> 00:11:17,450
and we're going to call getText in order to get each of them.

511
00:11:18,500 --> 00:11:21,560
This is the same as writing out a for loop like this

512
00:11:21,650 --> 00:11:26,180
but it's obviously much shorter. Now, when I hit run,

513
00:11:26,210 --> 00:11:28,880
you can see that each of my lists are ordered.

514
00:11:29,150 --> 00:11:33,950
So this is the first article's text, this is the first article's link,

515
00:11:34,220 --> 00:11:36,560
and this is the first article's points.

516
00:11:38,660 --> 00:11:43,660
What we want to do is we want to get the article_upvotes into a number format,

517
00:11:45,080 --> 00:11:47,750
so an integer. And to do that,

518
00:11:47,780 --> 00:11:51,380
we of course have to get rid of the points that comes afterwards.

519
00:11:51,830 --> 00:11:56,740
But notice how each of these items are strings. So that means we can split the string

520
00:11:56,890 --> 00:12:01,270
by the space and only get hold of the first item in that space.

521
00:12:02,110 --> 00:12:06,070
Let me show you what I mean. So we've got all of the article upvotes,

522
00:12:06,190 --> 00:12:09,670
let's go ahead and just print out the first item.

523
00:12:10,380 --> 00:12:11,213
Right.

524
00:12:13,770 --> 00:12:18,090
So now we just get the first item, which is 40 points. Now,

525
00:12:18,090 --> 00:12:21,840
if I take that item and I call the split method,

526
00:12:21,900 --> 00:12:26,400
then it's going to split every word in the sentence. By default,

527
00:12:26,400 --> 00:12:31,350
it splits by the space. Now, if I run this code,

528
00:12:32,220 --> 00:12:32,910
you can see

529
00:12:32,910 --> 00:12:37,910
I get a list where I've got the first item being 40 and the second being points.

530
00:12:39,390 --> 00:12:42,900
So it's basically split that string by the space.

531
00:12:43,740 --> 00:12:48,300
Now the next stage is I could get hold of just the first item that comes from

532
00:12:48,300 --> 00:12:50,130
that list, which is now 40.

533
00:12:50,820 --> 00:12:54,180
If I now finally wrap it around an int,

534
00:12:54,270 --> 00:12:59,130
then I can turn that into an actual number. Don't worry if your number changes

535
00:12:59,130 --> 00:13:03,600
because you're pulling data live from a website. That upvote number can change in

536
00:13:03,600 --> 00:13:06,540
any second. So this is the method

537
00:13:06,540 --> 00:13:10,260
how we can get hold of the actual number from the upvotes.

538
00:13:10,890 --> 00:13:14,790
Now we're going to apply all of this .split

539
00:13:14,850 --> 00:13:19,740
and also getting hold of the first item into our list comprehension.

540
00:13:20,100 --> 00:13:22,920
So for each of the scores that soup finds,

541
00:13:23,160 --> 00:13:27,420
we're going to get hold of the text and then split the text and then get the first

542
00:13:27,420 --> 00:13:30,150
item from the text. And then finally,

543
00:13:30,180 --> 00:13:35,180
we wrap all of this around an int and turn it into an integer. Then if I go ahead

544
00:13:36,960 --> 00:13:39,120
and uncomment all these lines of code,

545
00:13:39,540 --> 00:13:43,350
then you can see I've got all of these numbers being printed out,

546
00:13:43,980 --> 00:13:46,110
which means I can now sort them.

547
00:13:47,670 --> 00:13:51,930
I want to get the index of the list item that has the highest value.

548
00:13:52,140 --> 00:13:55,950
And then I want to use that index to pick out the title, text,

549
00:13:56,100 --> 00:13:58,920
and also the link from these two lists,

550
00:13:59,400 --> 00:14:02,610
because they're all ordered in exactly the same way.

551
00:14:02,610 --> 00:14:07,610
So this first item corresponds to this first link corresponds to this first 

552
00:14:07,950 --> 00:14:12,690
upvote. And I want to pose this to you as a challenge.

553
00:14:13,050 --> 00:14:16,080
Can you print out the title and link for the Hacker

554
00:14:16,080 --> 00:14:18,840
News story with the highest number of upvotes?

555
00:14:19,350 --> 00:14:22,080
Since we're working with three different lists at this point,

556
00:14:22,320 --> 00:14:27,320
you'll have to find the index of the largest number inside the article_upvotes

557
00:14:27,390 --> 00:14:28,830
list to accomplish this.

558
00:14:29,310 --> 00:14:32,910
I'll give you a few seconds to pause the video before I show you the solution.

559
00:14:36,330 --> 00:14:36,750
All right,

560
00:14:36,750 --> 00:14:41,750
here's the solution. We can use the max function that Python comes with to get

561
00:14:42,660 --> 00:14:46,440
the largest number from our article_upvotes.

562
00:14:48,450 --> 00:14:52,970
And then we can print this largest number and see if it works.

563
00:14:53,660 --> 00:14:58,040
So we've got 1,312. Now,

564
00:14:58,040 --> 00:15:00,650
once we've gotten hold of the largest number,

565
00:15:00,800 --> 00:15:04,160
then we can find its index from this list.

566
00:15:04,610 --> 00:15:07,640
So we can say article_upvotes.index

567
00:15:07,700 --> 00:15:11,300
and then we find the index of this largest number.

568
00:15:11,410 --> 00:15:12,243
Right.

569
00:15:16,270 --> 00:15:19,780
Now, if we hit run, you can see that we're getting index number 27.

570
00:15:20,470 --> 00:15:22,870
So instead of just printing out that index,

571
00:15:22,960 --> 00:15:27,960
we can print, instead, the article_texts with that index, so passing in the largest

572
00:15:30,010 --> 00:15:35,010
index, and also the article_links and passing in the same index.

573
00:15:37,690 --> 00:15:38,980
So now if I hit run,

574
00:15:39,070 --> 00:15:44,070
you can see that the most popular article at the moment on this page has this

575
00:15:45,100 --> 00:15:48,640
title text and this particular link. Of course for you

576
00:15:48,640 --> 00:15:52,870
it will be different because it depends on what's currently showing up on Hacker

577
00:15:52,870 --> 00:15:55,600
News. But if I refresh this page,

578
00:15:55,630 --> 00:16:00,630
you can see that this article with 1,313 points is of course the most popular

579
00:16:02,500 --> 00:16:05,890
article and it is the one that's about Mozilla.

580
00:16:07,030 --> 00:16:12,030
You can imagine a use case for this where every day we scrape all the data on Y

581
00:16:12,250 --> 00:16:16,840
Combinator and then we send ourselves through a text message through an email,

582
00:16:17,170 --> 00:16:20,860
the most upvoted title and article

583
00:16:21,100 --> 00:16:24,040
so that we can just look at that one thing.

584
00:16:25,150 --> 00:16:30,150
And you've seen now how we can use the requests module to get hold of the text

585
00:16:31,090 --> 00:16:34,750
the HTML code from a particular website,

586
00:16:35,020 --> 00:16:38,350
and then use Beautiful Soup to parse through that website

587
00:16:38,770 --> 00:16:43,770
and then to get hold of these specific parts that we want by using find_all or

588
00:16:44,470 --> 00:16:45,303
find,

589
00:16:45,460 --> 00:16:49,900
and then getting hold of the text or getting hold of the link or getting hold of

590
00:16:49,900 --> 00:16:51,640
any other thing that we want.

591
00:16:52,480 --> 00:16:56,920
So now that we've seen how we can do web scraping using Beautiful Soup, in the

592
00:16:56,920 --> 00:16:57,670
next lesson

593
00:16:57,670 --> 00:17:02,670
I want to talk a little bit about the ethics of scraping websites and when to do

594
00:17:03,370 --> 00:17:06,339
it and what you can use the data you get from this

595
00:17:06,339 --> 00:17:10,420
for. So for all of that and more, I'll see you on the next lesson.


596
00:00:00,510 --> 00:00:05,250
Now that we've learned how to do web scraping with requests and Beautiful Soup,

597
00:00:05,880 --> 00:00:10,880
it's time to step back for a moment and have a think about what we're allowed to

598
00:00:11,490 --> 00:00:16,490
do and what might not be a good idea when we're scraping data from other

599
00:00:16,710 --> 00:00:20,640
websites. Because after all, we don't own that data, right?

600
00:00:21,450 --> 00:00:26,370
When you think about services like Google or Bing or any other search engine,

601
00:00:26,790 --> 00:00:31,790
essentially what they're doing is they're constantly scraping data from all of the

602
00:00:32,940 --> 00:00:35,280
websites that are listed on the internet.

603
00:00:35,850 --> 00:00:39,780
And that's how they manage to get the information about what's on each page

604
00:00:40,110 --> 00:00:43,740
and for it to show up for users who use their search service.

605
00:00:44,490 --> 00:00:49,490
Now we have to step back for a moment and think about what is the law on web

606
00:00:49,920 --> 00:00:52,530
scraping? What is legal and what is illegal?

607
00:00:53,250 --> 00:00:55,530
Even as we were looking at Hacker News

608
00:00:55,530 --> 00:00:59,850
just now I noticed that one of the articles in fact talks about the Genius law

609
00:00:59,880 --> 00:01:04,170
suit with Google. And in terms of recent history,

610
00:01:04,170 --> 00:01:08,310
there's two really famous cases, which is Genius suing Google

611
00:01:08,640 --> 00:01:12,810
because they're saying that Google is scraping the song lyrics from their

612
00:01:12,810 --> 00:01:17,340
website and they're actually displaying it without taking people to Genius.

613
00:01:17,910 --> 00:01:21,270
So for example, if we're looking at the lyrics for Code m
Monkey,

614
00:01:21,930 --> 00:01:26,930
you can see that Google automatically shows the lyrics straight inside of

615
00:01:26,940 --> 00:01:27,773
Google.

616
00:01:27,810 --> 00:01:32,760
That means that a user can potentially just get all the information they need,

617
00:01:33,120 --> 00:01:34,410
say all of the song

618
00:01:34,410 --> 00:01:39,410
lyrics to this song, without ever needing to visit the website where this lyric

619
00:01:39,960 --> 00:01:40,793
might come from.

620
00:01:41,430 --> 00:01:45,600
And that lyric might've been compiled by somebody on Genius.

621
00:01:46,170 --> 00:01:50,190
Genius has a lyric annotation website. And of course,

622
00:01:50,280 --> 00:01:51,510
as with all websites,

623
00:01:51,510 --> 00:01:56,510
they rely on users actually visiting their website to make money or to show ads.

624
00:01:57,630 --> 00:02:00,570
And if Google simply just shows it in their search results,

625
00:02:00,870 --> 00:02:03,930
then this can be a problem for websites like Genius.

626
00:02:04,470 --> 00:02:09,470
So they sued them over this and actually ended up losing the lawsuit.

627
00:02:10,560 --> 00:02:15,560
Another really famous example of a lawsuit over scraping is hiQ versus

628
00:02:17,130 --> 00:02:22,080
LinkedIn. So hiQ was scraping data from LinkedIn to use commercially.

629
00:02:22,620 --> 00:02:27,360
So LinkedIn sued them and ended up losing in the lawsuit.

630
00:02:28,050 --> 00:02:29,250
Based on these lawsuits,

631
00:02:29,280 --> 00:02:33,960
we have a little bit of a better idea of what is legal when it comes to web

632
00:02:33,960 --> 00:02:36,180
scraping and what is not legal.

633
00:02:36,780 --> 00:02:41,460
The law actually seems to favor web scraping in the sense that you're allowed to

634
00:02:41,460 --> 00:02:43,350
scrape a website data

635
00:02:43,920 --> 00:02:47,430
as long as you think about a couple of things.

636
00:02:48,090 --> 00:02:53,090
A lot of people have been writing about web scraping being legal based on the

637
00:02:53,370 --> 00:02:55,860
LinkedIn versus hiQ case.

638
00:02:56,250 --> 00:03:01,060
But the important thing to remember is that this is not a blanket sort of,

639
00:03:01,300 --> 00:03:04,690
you can do whatever you want, scraping any website's data.

640
00:03:05,320 --> 00:03:10,320
It only means that data that is publicly available and not copyrighted is

641
00:03:11,560 --> 00:03:16,390
probably legal for companies to scrape. Now,

642
00:03:16,420 --> 00:03:18,400
if you are using this data privately

643
00:03:18,400 --> 00:03:22,510
like we are creating some sort of service for ourselves, then it doesn't really

644
00:03:22,510 --> 00:03:24,160
matter. You're  just a user.

645
00:03:24,820 --> 00:03:28,150
The difficulty comes when you're trying to commercialize that data,

646
00:03:28,150 --> 00:03:32,590
when you set up a business and your business kind of involves somebody else's

647
00:03:32,590 --> 00:03:35,890
data. That is a bit of a gray area. Now,

648
00:03:35,950 --> 00:03:38,170
the things that we definitely know are

649
00:03:38,170 --> 00:03:40,720
that you can't commercialize copyrighted content.

650
00:03:40,990 --> 00:03:45,790
So if you scrape data from YouTube and you scraped the video data,

651
00:03:45,820 --> 00:03:50,820
you can't just use that video on your own website. That is still not allowed

652
00:03:51,040 --> 00:03:56,040
because that video is copyrighted and it's created by a YouTube user and the

653
00:03:56,650 --> 00:04:00,820
copyright belongs to that user, not to you. So this is still illegal.

654
00:04:01,600 --> 00:04:05,620
This might also apply to other things like a Medium blog post that somebody else

655
00:04:05,620 --> 00:04:09,310
wrote or a piece of music that's being hosted on Spotify.

656
00:04:09,760 --> 00:04:12,430
So copyrighted content you can't commercialize.

657
00:04:13,030 --> 00:04:17,110
The second thing is that you can't scrape data that's behind authentication.

658
00:04:17,470 --> 00:04:21,100
So if you have to log into Facebook in order to scrape the data,

659
00:04:21,310 --> 00:04:22,810
then that's pretty much illegal.

660
00:04:23,380 --> 00:04:27,460
And the reason for this is when you sign up as a user to any of these services

661
00:04:27,460 --> 00:04:30,400
like Facebook or Twitter or Instagram,

662
00:04:30,820 --> 00:04:35,020
there's a policy in there that you are agreeing to when you sign up that says

663
00:04:35,050 --> 00:04:39,610
I agree to not use this data that I obtained on this website commercially.

664
00:04:40,180 --> 00:04:43,120
But the data that is not behind authentication,

665
00:04:43,420 --> 00:04:46,720
so any website that you can access as it is,

666
00:04:47,140 --> 00:04:51,490
they can't bind you to a policy because you haven't agreed to anything.

667
00:04:51,970 --> 00:04:56,260
So if the website has data that just out there in the open that you can access

668
00:04:56,260 --> 00:05:00,430
without logging in and the content is not something that can be copyrighted,

669
00:05:00,670 --> 00:05:03,640
then it is fair game legally. Now,

670
00:05:03,670 --> 00:05:07,780
just because it's legal doesn't mean that you can actually do it.

671
00:05:08,350 --> 00:05:13,350
A lot of websites will use captcha or recaptcha in order to prevent bots like

672
00:05:13,750 --> 00:05:18,610
our Python code to get data from their websites. Every single time,

673
00:05:18,640 --> 00:05:21,850
you're agreeing to one of these captchas, it's testing

674
00:05:21,850 --> 00:05:24,100
whether to see if your actually a real human

675
00:05:24,340 --> 00:05:28,840
or if you just a bit of code that is trying to access their data. Captcha was the

676
00:05:28,840 --> 00:05:33,340
old version where you had the type in some squiggle letters and recaptcha is the

677
00:05:33,340 --> 00:05:36,130
new version where you just have to tick a checkbox.

678
00:05:36,460 --> 00:05:38,560
And it's actually really interesting how it works.

679
00:05:39,400 --> 00:05:43,210
It looks at things like how your mouse approaches the checkbox,

680
00:05:43,210 --> 00:05:47,020
how you maybe quiver a little bit before you actually check it

681
00:05:47,260 --> 00:05:51,280
and other things like your cookies and the store data that they have on you.

682
00:05:51,970 --> 00:05:56,170
Essentially, this service is used by websites to prevent people

683
00:05:56,230 --> 00:06:00,590
to scrape their data using a bot. The other thing to remember is that,

684
00:06:00,830 --> 00:06:03,560
you know, if you get sued by somebody like LinkedIn

685
00:06:03,560 --> 00:06:08,420
because you're using their data and you're building a business on it

686
00:06:08,450 --> 00:06:10,970
like hiQ is, then you can

687
00:06:10,970 --> 00:06:14,060
at any moment be hit with a really expensive lawsuit

688
00:06:14,450 --> 00:06:18,620
and you are going to have to pay a lot of money to lawyer up in order to contest

689
00:06:18,620 --> 00:06:20,420
this and actually to fight them in court.

690
00:06:20,930 --> 00:06:25,400
Unless you have the money to lawyer up and fight a company like LinkedIn,

691
00:06:26,000 --> 00:06:29,810
it's really important to know what are the implications of web scraping,

692
00:06:29,930 --> 00:06:33,590
especially when you're selling that data as a part of your business.

693
00:06:34,250 --> 00:06:37,040
But in addition to the sort of legal side of things,

694
00:06:37,190 --> 00:06:40,940
the other part that you should really think about is the ethics of web scraping.

695
00:06:41,390 --> 00:06:44,810
This is basically putting aside what is legal and what is illegal,

696
00:06:45,020 --> 00:06:46,640
but more thinking about what is right

697
00:06:46,640 --> 00:06:51,640
and what's wrong because let's say that you've built a website and you've got

698
00:06:51,770 --> 00:06:56,000
some sort of bot that's constantly scraping it for data, data that you know,

699
00:06:56,300 --> 00:06:58,730
has been generated by your own users

700
00:06:58,970 --> 00:07:03,950
that's really precious and that you might even charge for it, then,

701
00:07:03,980 --> 00:07:07,400
is it really right for somebody to do that?

702
00:07:07,940 --> 00:07:11,780
So I often follow the rule where if I don't want something to happen to me,

703
00:07:11,840 --> 00:07:16,160
I try to not do that to others. In terms of the ethics, a couple of things

704
00:07:16,170 --> 00:07:18,470
I would recommend abiding by is

705
00:07:18,770 --> 00:07:21,800
if you come across a website and they have a public API

706
00:07:21,860 --> 00:07:26,210
which we've already learned about and we know how to use, then always

707
00:07:26,210 --> 00:07:30,770
always go for the API. If it requires an application, then apply for it.

708
00:07:31,100 --> 00:07:35,570
Don't just go ahead and try to take their data when there's already a route for

709
00:07:35,570 --> 00:07:37,310
you to use and access their data.

710
00:07:38,480 --> 00:07:42,590
The second thing is to respect the web owner, because you know,

711
00:07:42,590 --> 00:07:46,550
you don't want somebody to access your website a million times a second,

712
00:07:46,610 --> 00:07:49,520
potentially making your website go down

713
00:07:49,670 --> 00:07:51,680
or it could count as a DDoS

714
00:07:51,680 --> 00:07:55,310
attack where it affects other users using the website.

715
00:07:56,090 --> 00:07:57,290
When you are on a website,

716
00:07:57,590 --> 00:08:02,360
they actually provide a way for you to tell what it is that you can scrape and

717
00:08:02,360 --> 00:08:02,810
what it is

718
00:08:02,810 --> 00:08:07,810
you can't. At the very end of the URLs after the.com or.co.uk,

719
00:08:08,930 --> 00:08:13,220
if you put a forward slash and put robots.txt, you can see

720
00:08:13,220 --> 00:08:18,220
this is the advice that they give to any bots that are potentially scraping

721
00:08:18,260 --> 00:08:19,093
their website.

722
00:08:19,610 --> 00:08:24,610
User agent is the person who is scraping, the person or the bot that's scraping,

723
00:08:25,280 --> 00:08:27,890
and it tells you what are the things that it disallows.

724
00:08:28,220 --> 00:08:32,690
So it doesn't want you to access the /vote?, /reply?, 

725
00:08:32,690 --> 00:08:35,299
/submitted?, /threads?.

726
00:08:35,600 --> 00:08:39,950
So basically any of these end points are ones that they don't really want you to

727
00:08:39,950 --> 00:08:41,840
use. For example,

728
00:08:41,840 --> 00:08:45,050
here I've access the /reply?

729
00:08:45,380 --> 00:08:48,890
which is a way to log in and reply to a particular comment.

730
00:08:49,280 --> 00:08:51,740
Now that really shouldn't be a bot kind of action

731
00:08:51,740 --> 00:08:54,230
because then it means the data that's generated

732
00:08:54,530 --> 00:08:57,690
or the replies on here will be automated, right?

733
00:08:57,690 --> 00:09:01,980
You actually want humans to comment and reply on the articles rather than some

734
00:09:01,980 --> 00:09:02,813
sort of robot.

735
00:09:03,660 --> 00:09:07,590
So these are the paths that they don't want you to access as a bot.

736
00:09:08,040 --> 00:09:10,890
And finally, it even tells you a crawl-delay.

737
00:09:10,920 --> 00:09:15,630
So this is the number of seconds that you should leave between each time you hit

738
00:09:15,630 --> 00:09:16,463
up the website.

739
00:09:17,250 --> 00:09:22,200
If we're writing Python code and we're using Beautiful Soup and response to

740
00:09:22,200 --> 00:09:24,210
scrape data from YCombinator,

741
00:09:24,480 --> 00:09:28,590
we could potentially get that code to run every fraction of a second right?

742
00:09:28,590 --> 00:09:33,450
I could just write a for loop and just get this to keep scraping again and again

743
00:09:33,450 --> 00:09:34,283
and again.

744
00:09:34,350 --> 00:09:38,880
But that means that you're adding a lot of extra traffic and a lot of extra

745
00:09:38,880 --> 00:09:43,560
demand on their servers which could potentially mean that real users,

746
00:09:43,560 --> 00:09:48,560
real humans who want to access their website might not be able to do it at a fast

747
00:09:48,780 --> 00:09:51,090
speed. So this is the reason why

748
00:09:51,120 --> 00:09:53,880
when a lot of people accessing the same website,

749
00:09:53,910 --> 00:09:58,910
say when a new ticket has been released for Glastonbury or some sort of big

750
00:09:59,100 --> 00:10:01,950
concert, that the website can go down.

751
00:10:02,010 --> 00:10:05,430
Its because a lot of servers can't cope with so much demand.

752
00:10:05,850 --> 00:10:08,070
And when that demand is coming from a for loop,

753
00:10:08,340 --> 00:10:12,150
then you can imagine that you're just adding a lot of extra work onto the web

754
00:10:12,150 --> 00:10:15,480
server. So always respect their crawl-delay

755
00:10:15,480 --> 00:10:20,190
if you see one in the robots.txt, and even if you don't see one,

756
00:10:20,280 --> 00:10:24,450
just try to limit your rate so that you don't max out their server.

757
00:10:24,840 --> 00:10:27,450
I recommend not scraping more than once a minute.

758
00:10:28,200 --> 00:10:32,340
The YCombinator's of robots.txt is actually quite permissive.

759
00:10:32,370 --> 00:10:35,430
It allows you to do pretty much anything you want,

760
00:10:35,760 --> 00:10:37,950
but that's not true for all websites.

761
00:10:38,160 --> 00:10:40,260
If you look at the robots.txt for LinkedIn,

762
00:10:40,590 --> 00:10:43,770
you can see that they really don't want anyone to scrape it.

763
00:10:43,770 --> 00:10:45,450
There is a bit of legal jargon,

764
00:10:45,480 --> 00:10:49,470
there's a lot more disallows that you can see, right?

765
00:10:49,950 --> 00:10:53,820
This is probably not a website where I would scrape their data and try to build a

766
00:10:53,820 --> 00:10:54,690
company around.

767
00:10:55,620 --> 00:10:59,940
So remember that this is a piece of text that the website owners have written

768
00:11:00,180 --> 00:11:04,920
for you to look at to see what you can do and you can't do with their website.

769
00:11:05,280 --> 00:11:06,990
So before you scrape a website,

770
00:11:07,320 --> 00:11:12,320
always go to the root of their URL and check out their robots.txt and follow

771
00:11:14,490 --> 00:11:18,420
the ethical codes of conduct when you're trying to commercialize a project.

772
00:11:18,810 --> 00:11:22,770
So this is just the quick tip on the law and ethics of web scraping

773
00:11:22,980 --> 00:11:24,960
just so that you don't get into trouble in the future.


774
00:00:00,690 --> 00:00:01,859
Now here's a question.

775
00:00:02,310 --> 00:00:07,310
Have you ever gotten into a situation where it's evening time and you wanna

776
00:00:07,920 --> 00:00:10,710
watch a movie, but you don't know what to watch?

777
00:00:11,160 --> 00:00:15,000
Like there's just no ideas coming to your mind and everybody's sitting around

778
00:00:15,000 --> 00:00:16,500
the TV, like, what do we do?

779
00:00:17,400 --> 00:00:22,400
So what if we scraped a list of the top 100 movies of all time and you pick one

780
00:00:25,320 --> 00:00:26,153
from there?

781
00:00:26,700 --> 00:00:31,700
So I recently came across this article on empire where they list the 100

782
00:00:32,040 --> 00:00:34,590
greatest movies of all time.

783
00:00:35,220 --> 00:00:40,220
And you can see that it goes from 100 all the way down to one.

784
00:00:41,820 --> 00:00:45,960
These are supposed to be the top rated movies that have ever been produced

785
00:00:46,440 --> 00:00:51,440
and I always wanted to watch through all 100 of them and check off each one as I

786
00:00:51,810 --> 00:00:56,790
go along. So this is what we're going to be doing for our final project today.

787
00:00:57,180 --> 00:01:02,040
You're going to be scraping from this website the 100 movies

788
00:01:02,430 --> 00:01:06,570
and you are going to be using Python code to create a text file called movies

789
00:01:06,570 --> 00:01:11,040
.text that lists them in order, starting from one.

790
00:01:11,760 --> 00:01:16,760
Notice how each of these are just the titles of each of these movies.

791
00:01:17,730 --> 00:01:20,160
Essentially, it's this part that you want.

792
00:01:20,730 --> 00:01:23,520
And because the listing starts from 100,

793
00:01:23,790 --> 00:01:27,090
you have to figure out how to flip it the other way so that you get it

794
00:01:27,090 --> 00:01:30,270
starting from one. But essentially this is the goal.

795
00:01:30,450 --> 00:01:32,910
And this is the project for today.

796
00:01:32,970 --> 00:01:35,190
You're going to have to use what you learned about web scraping,

797
00:01:35,460 --> 00:01:38,580
but also other things that you've learned along the way about Python.

798
00:01:39,330 --> 00:01:42,540
Pause the video now and try to complete this challenge.

799
00:01:45,230 --> 00:01:45,770
Okay.

800
00:01:45,770 --> 00:01:49,340
So here I've created a brand new blank project,

801
00:01:49,370 --> 00:01:52,670
which I've called top100-movies, but of course, that doesn't matter.

802
00:01:53,210 --> 00:01:56,900
But in the main.py is where we're going to be doing all the serious stuff.

803
00:01:57,200 --> 00:02:01,160
So the first thing we'll need is to copy the URL.

804
00:02:01,670 --> 00:02:05,870
And at the time when you're doing this project, this URL might change.

805
00:02:06,110 --> 00:02:08,090
So don't get it from here. Instead,

806
00:02:08,360 --> 00:02:11,480
go to the course resources and copy the URL from there.

807
00:02:12,380 --> 00:02:13,790
Once you've got the URL,

808
00:02:13,820 --> 00:02:18,020
we're going to paste it into our main.py and save it as a constant.

809
00:02:18,710 --> 00:02:21,980
In addition, we're going to have to import all the things that we need,

810
00:02:22,010 --> 00:02:23,480
including requests,

811
00:02:23,870 --> 00:02:27,500
and also the bs4 module

812
00:02:28,790 --> 00:02:31,280
where we can get hold of our Beautiful Soup.

813
00:02:33,980 --> 00:02:37,640
I'm going to need to install both of these because I don't have them yet.

814
00:02:38,510 --> 00:02:42,440
So I'm going to click on the red squiggly line, install requests,

815
00:02:43,070 --> 00:02:47,330
and also install beautiful soup. Once that's all done,

816
00:02:47,390 --> 00:02:52,390
then my red squiggly lines should be gone and I can now start using my request

817
00:02:52,880 --> 00:02:53,870
module to

818
00:02:53,870 --> 00:02:58,870
make a get request to this particular URL. And I'll save the response

819
00:02:59,100 --> 00:03:01,750
I get back as the response.

820
00:03:03,280 --> 00:03:05,200
The actual HTML files,

821
00:03:05,200 --> 00:03:10,200
so the website HTML, is actually under response.text.

822
00:03:11,560 --> 00:03:14,440
So this is going to be the raw HTML text,

823
00:03:14,800 --> 00:03:18,820
and this is what I'm going to be using to use Beautiful Soup to parse.

824
00:03:19,480 --> 00:03:24,010
So now we make soup and we're going to use Beautiful Soup and we're going to

825
00:03:24,010 --> 00:03:26,110
parse in our website HTML

826
00:03:26,530 --> 00:03:29,170
and also the html.parser

827
00:03:29,620 --> 00:03:33,220
which is the method which we're going to use to parse through this website.

828
00:03:34,360 --> 00:03:35,710
Now that we've made soup,

829
00:03:35,740 --> 00:03:40,720
let's go ahead and print out our soup in a predefined format so that we can just

830
00:03:40,720 --> 00:03:41,980
see what it looks like.

831
00:03:42,160 --> 00:03:46,660
So let's run our main.py and we can see that we've got basically that

832
00:03:46,660 --> 00:03:49,750
entire website's HTML being printed out

833
00:03:49,750 --> 00:03:54,750
here. Now comes the part where we need to use our Google Chrome inspector.

834
00:03:56,020 --> 00:04:00,700
The part that we want from this website is just these lines. Now,

835
00:04:00,730 --> 00:04:02,500
of course, if we didn't know how to code,

836
00:04:02,740 --> 00:04:06,490
we would be here copying and pasting for hours on end,

837
00:04:06,910 --> 00:04:11,890
and we would die of boredom or we'd get repetitive strain injury from doing so

838
00:04:11,890 --> 00:04:15,760
much copy and pasting. But because we know code, we know better than that.

839
00:04:15,910 --> 00:04:19,510
So let's get hold of the part that we want. Let's right-

840
00:04:19,510 --> 00:04:22,270
click and click inspect. Now,

841
00:04:22,270 --> 00:04:27,220
we can see that this lives inside and h3  with the class of title.

842
00:04:27,790 --> 00:04:31,630
Now let's just check against one of the other pieces that we want and just make

843
00:04:31,630 --> 00:04:33,880
sure that they've got the same structure.

844
00:04:34,330 --> 00:04:38,470
So this is also inside an h3 with the class of title.

845
00:04:38,950 --> 00:04:43,180
So basically as long as we can scrape this entire page and get all of the 

846
00:04:43,210 --> 00:04:48,040
h3 with the class of title and get the text that's contained inside the 

847
00:04:48,070 --> 00:04:52,120
h3 element, then we're golden. Let's go ahead and do that.

848
00:04:52,600 --> 00:04:55,120
So instead of printing our soup.prettify,

849
00:04:55,150 --> 00:04:57,550
I'm going to tap into soup and I'm going to say

850
00:04:57,550 --> 00:05:02,550
find all. The thing that I want to find has the tag name of an h3 and it

851
00:05:04,360 --> 00:05:07,300
has the class of title.

852
00:05:08,080 --> 00:05:11,710
So that all came from our inspections right here.

853
00:05:12,400 --> 00:05:15,940
This should get us a list of all of the 

854
00:05:15,970 --> 00:05:17,620
h3 elements with this class

855
00:05:18,160 --> 00:05:23,110
and we should be able to save that into a variable called all_movies.

856
00:05:23,980 --> 00:05:28,450
Let's print all_movies and see what we get.

857
00:05:30,190 --> 00:05:33,430
Now, we've got a list of all of our h3s,

858
00:05:33,970 --> 00:05:38,970
and we're now going to go one step further and fetch the text from within these

859
00:05:41,380 --> 00:05:45,340
h3 elements. We do that using the

860
00:05:45,640 --> 00:05:49,000
getText method. But we can't do that on the list

861
00:05:49,030 --> 00:05:51,820
so we're going to use list comprehension.

862
00:05:52,390 --> 00:05:57,390
So we're going to say the movie_titles is equal to a new list

863
00:05:58,850 --> 00:05:59,780
and in this list,

864
00:05:59,930 --> 00:06:04,930
each item is going to be formed from a movie in the all_movies list.

865
00:06:09,710 --> 00:06:13,970
And this new item is going to be created by taking each of the movies in the

866
00:06:13,970 --> 00:06:17,900
list and then calling getText on it. Now,

867
00:06:17,900 --> 00:06:21,380
if I go ahead and print my movie_titles

868
00:06:21,410 --> 00:06:25,940
instead of all_movies, then this is what we get.

869
00:06:25,940 --> 00:06:29,930
We get all of the titles of all 100 movies.

870
00:06:32,120 --> 00:06:37,120
Now we want to reverse this list so that we can put it into a text file starting

871
00:06:38,060 --> 00:06:40,490
from 1, going down to 100.

872
00:06:41,000 --> 00:06:43,070
So there's a couple of ways that we could do this.

873
00:06:43,130 --> 00:06:46,520
One is we can use the Python splice operator.

874
00:06:46,910 --> 00:06:49,100
So we add a set of square brackets

875
00:06:49,520 --> 00:06:52,220
and then we add a ::-1,

876
00:06:54,650 --> 00:06:58,400
and this will reverse the order. And as always,

877
00:06:58,760 --> 00:07:03,260
you can find out this information either by Googling or through what you've

878
00:07:03,260 --> 00:07:07,700
learned before in previous lessons. So this is the syntax that we're using,

879
00:07:08,180 --> 00:07:12,380
which comes from the slice operator where we have a start,

880
00:07:12,680 --> 00:07:15,620
a stop, and a step. So in this case,

881
00:07:15,800 --> 00:07:18,410
the start is at the very beginning of the list,

882
00:07:18,470 --> 00:07:20,330
the stop is at the very end of the list so

883
00:07:20,360 --> 00:07:22,940
we don't have to specify those cause they're the defaults,

884
00:07:23,360 --> 00:07:28,360
and then the step is basically -1 and this syntax will reverse that list.

885
00:07:31,040 --> 00:07:32,480
Now, alternatively, you can,

886
00:07:32,480 --> 00:07:37,480
of course also use a for loop where you create some sort of n in a range and the

887
00:07:40,370 --> 00:07:44,150
range again, can take a start, stop and step.

888
00:07:44,630 --> 00:07:46,820
So you could start at the end,

889
00:07:46,850 --> 00:07:50,930
so the length of our movie_titles-1

890
00:07:51,020 --> 00:07:55,370
because remember lists in Python start numbering from zero.

891
00:07:55,730 --> 00:08:00,680
So the very last item is actually at the total number minus one.

892
00:08:01,370 --> 00:08:05,690
And next is going to be the end, which is going to be zero, and finally

893
00:08:05,690 --> 00:08:09,770
it's going to be the step, which is minus one each time.

894
00:08:09,890 --> 00:08:13,790
So this time we start off from the very end of the list,

895
00:08:13,790 --> 00:08:18,620
go back to the beginning, stepping by minus one each time. And this way,

896
00:08:18,620 --> 00:08:19,880
if we print n,

897
00:08:19,940 --> 00:08:24,940
you can see that this is going to give us basically all the way from 99 down to

898
00:08:26,090 --> 00:08:26,923
1.

899
00:08:27,590 --> 00:08:32,590
And we can use that to tap into our movie titles and get hold of each of the items

900
00:08:35,330 --> 00:08:36,620
at index n.

901
00:08:41,110 --> 00:08:45,580
And because range actually doesn't go beyond the end

902
00:08:45,610 --> 00:08:50,110
we actually have to put -1 there if we want to get the very final one

903
00:08:50,110 --> 00:08:55,000
which is number 100. And you'll notice also that there's a

904
00:08:55,200 --> 00:08:58,110
bit of a typo here for number 93,

905
00:08:58,530 --> 00:09:00,990
and this is actually not our fault at all.

906
00:09:00,990 --> 00:09:04,380
It's in fact, in the original empire post.

907
00:09:04,440 --> 00:09:09,440
They actually screwed up and this should be number 93. Because we're scraping

908
00:09:10,650 --> 00:09:12,150
data we can't really be picky.

909
00:09:12,480 --> 00:09:15,090
We're just going to end up with what we end up with.

910
00:09:15,870 --> 00:09:20,870
So I'm going to choose the method where we actually take the movie title

911
00:09:20,910 --> 00:09:25,910
and then we use the splice to get hold of it in reverse.

912
00:09:27,090 --> 00:09:29,460
And I'll call that the movies.

913
00:09:30,690 --> 00:09:35,640
And now we can create our new text file. So with open

914
00:09:35,730 --> 00:09:38,670
we're going to create a new file called movies.txt,

915
00:09:39,090 --> 00:09:44,090
and we of course have to change the mode two write mode so that we can actually

916
00:09:44,490 --> 00:09:47,370
create this file. And then

917
00:09:47,430 --> 00:09:51,810
because this file movies.txt doesn't exist, once this line runs

918
00:09:51,810 --> 00:09:55,230
it's going to create that file and then we're going to write to it.

919
00:09:55,230 --> 00:09:57,060
So we're going to say file.write

920
00:09:57,510 --> 00:10:02,510
and we're going to write each of the lines in our list of movies.

921
00:10:03,060 --> 00:10:05,160
We can again use a for loop

922
00:10:05,460 --> 00:10:08,370
so for movie in movies,

923
00:10:08,850 --> 00:10:13,850
let's go ahead and write the name of the movie

924
00:10:16,800 --> 00:10:20,040
and then lets add a new line operator,

925
00:10:20,040 --> 00:10:25,040
so \n, so that we get each of the movies onto its own line.

926
00:10:26,250 --> 00:10:28,710
Now, finally, if I run this code,

927
00:10:28,830 --> 00:10:32,790
then it's going to create our movies.txt. And if I take a look at it,

928
00:10:32,820 --> 00:10:37,820
you can see it's now got all of the 100 movies listed on here from 1 down to

929
00:10:38,250 --> 00:10:39,083
100.

930
00:10:39,900 --> 00:10:43,710
And now you can go through this list and delete the ones that you've already

931
00:10:43,710 --> 00:10:47,970
watched and then continue watching through the rest of the list.

932
00:10:48,660 --> 00:10:52,470
I hope you had fun trying out web scraping by yourself in this project.

933
00:10:52,740 --> 00:10:56,100
So that's all for today. Take a look at what you've done.

934
00:10:56,130 --> 00:11:00,600
If there's anything confusing about the class or ID or HTML,

935
00:11:00,750 --> 00:11:04,410
then be sure to review some of the lessons in the previous four days.

936
00:11:05,400 --> 00:11:06,420
That's all for today.


